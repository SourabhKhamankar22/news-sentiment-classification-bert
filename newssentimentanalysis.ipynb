{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "986fd2ed3a1749f48edb9be0fbc4057c",
      "03e6594bd7ba460bb504b5602af5cd3b",
      "3db5498b77484a0e96a64770489db3d0",
      "52c8da3a552249e58c09156c3eff82a0",
      "1a29a51964ca42ec815a4033746c0864",
      "78409a83deb548f39e9a80ca706ad953",
      "089b7f832e7c4839852d90a9ae0148cf",
      "f33cf7e63e6543a89dd3f59b026e7f76",
      "478baf719fe2480cb0da90b01a5d608c",
      "67071cc660fc4a14b56f7a39fb2807f4",
      "6d21616965e2464186ff77aea8fda04e",
      "c0160fd9a27c48feaf2d743d56c094f5",
      "d4e33bdbe74e4b5d99f6f6ae401f7eeb",
      "c1087714cd7c47fc9d608afa09a6051e",
      "cd6f3f62ffe7424895c6462df8ee7bdf",
      "1d3cc314eb9041d69867cfbe39ede37e",
      "64bfe10b34964a1ca3f23d4b345f395f",
      "57c8c8fe763a48eab1277a92bd99f623",
      "133e846751e24f8cac30e233dc2401cf",
      "56347f9809f248f69352bf8a6975d130",
      "d6b5b98564b948ef8b23743810945189",
      "1b3cce9e7f114571b45c50380e2d9847",
      "120d84ee5fce4ef9a9c7e072ac6d3929",
      "d99648c490104d8e9d3e47dfddb05cd7",
      "2448f7d218f74ccdbfbbd483d17ba9e7",
      "26eb605b69a34e388a012a0d082fc881",
      "314a5ec8b9c24554bdf33278c6b925b2",
      "bb2daeb1d2b847378095bcf800214f98",
      "5309b7e2688949fa904b1f4a19b4d772",
      "cc7b94a8e7e34502b3bbaeb3572ca2ae",
      "092449de7d044065a366d26273efe2d4",
      "2837ae6b503149bbaf64371842a0ffb5",
      "0ac10ccb11eb424d8c174bd18f3426ac",
      "6a3c96d3af8e4a9dbf2e61915458a12a",
      "dfefef77952c4f05a490a2ca6fa366c9",
      "f466e52055e7412b8483d6cdd146186d",
      "8e2c171aa0ed4577a35c11f7efdb234f",
      "e3d0d20d6d8d49d3a2d8fc4c14d0a86f",
      "696ff06a156d414292a0511bcfd97844",
      "2df72f6f9f3d430aa9153d8f5df8170d",
      "08838b4c279f4820a601e98d4dc2d2bd",
      "3bcc8a346aed4e07b7ce59a342044d90",
      "1eace41ce63c459b9dac24412fabb009",
      "dd166dc3250b4c80af5395351f2e26d1",
      "510f7583e4ea479a8799d274f2c29098",
      "aeca8679d14d4f8a8a0bd4de0fc55295",
      "06d99fb71fe9433f8c66f4e55c3e08d6",
      "f0783b5a9c174b4d8d34cac87cad1450",
      "9ef88de36dce4714a4731d37ff3d5617",
      "618b3d7ed356472fbc4aafa49630ef4e",
      "72ef5e6e895e41d58adae9a73b3973a8",
      "1bbea9dc7e80470f9b62d7a70649ef97",
      "75c76487508c4a2598489df4e5d06be3",
      "0192be89f39441e7897854d835b1685e",
      "4d17d5fed34f432b851fa29841ff4cdf"
     ]
    },
    "id": "tg94a_T-NOPL",
    "outputId": "1c3a39d3-f96b-4bd9-d771-4e80809fb761"
   },
   "outputs": [],
   "source": [
    "# --- Install necessary packages ---\n",
    "!pip install transformers datasets torch scikit-learn fastapi uvicorn streamlit seaborn safetensors\n",
    "\n",
    "# --- Import required libraries ---\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "# --- Seed control for reproducibility ---\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Check available device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load the VERIFIED dataset and drop NaN values ---\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "df = pd.read_csv(\"news_sentiment_corrected_verified_fully_balanced.csv\")\n",
    "df.dropna(subset=[\"Description\", \"verified_sentiment\"], inplace=True)\n",
    "\n",
    "# --- Text preprocessing ---\n",
    "df[\"cleaned_text\"] = df[\"Description\"].astype(str).str.lower()\n",
    "\n",
    "# --- Sentiment mapping ---\n",
    "sentiment_mapping = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
    "df[\"verified_sentiment\"] = df[\"verified_sentiment\"].str.lower().map(sentiment_mapping)\n",
    "df.dropna(subset=[\"verified_sentiment\"], inplace=True)\n",
    "\n",
    "# --- Prepare texts and labels ---\n",
    "labels = df[\"verified_sentiment\"].tolist()\n",
    "texts = df[\"cleaned_text\"].tolist()\n",
    "\n",
    "# --- Tokenizer ---\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# --- Train/test split ---\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Tokenize ---\n",
    "train_encodings = tokenizer(train_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# --- Dataset class ---\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = {k: torch.tensor(v, dtype=torch.long) for k, v in encodings.items()}\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "# --- Create datasets ---\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels)\n",
    "\n",
    "# --- Model config with lower dropout ---\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=3, hidden_dropout_prob=0.2)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config).to(device)\n",
    "\n",
    "# --- TrainingArguments with lower learning rate ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=300,\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# --- Metrics ---\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"precision\": precision_score(labels, preds, average=\"macro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"macro\"),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Evaluate ---\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "# --- Save model and tokenizer ---\n",
    "model.save_pretrained(\"./bert_sentiment_model_verified\")\n",
    "tokenizer.save_pretrained(\"./bert_sentiment_model_verified\")\n",
    "\n",
    "# --- Predict Function ---\n",
    "def predict_sentiment(text: str):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"./bert_sentiment_model_verified\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"./bert_sentiment_model_verified\").to(device)\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    pred_label = torch.argmax(probs, dim=1).item()\n",
    "    confidence = probs[0][pred_label].item()\n",
    "    return {\"sentiment\": label_map[pred_label], \"confidence\": round(confidence, 4)}\n",
    "\n",
    "# --- Example prediction ---\n",
    "print(predict_sentiment(\"The economy is booming and people are happy!\"))\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "print(classification_report(test_labels, y_pred))\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "id": "HSBljAfEmvGh",
    "outputId": "d8589757-81bd-4efd-e9ff-bb3186ebef71"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step-wise values\n",
    "steps = [300, 600, 900, 1200]\n",
    "train_loss = [0.500000, 0.208200, 0.110600, 0.063900]\n",
    "val_loss = [0.420146, 0.302414, 0.402462, 0.321273]\n",
    "accuracy = [0.858790, 0.904899, 0.893372, 0.925072]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.plot(steps, train_loss, label='Training Loss', marker='o', color='blue')\n",
    "plt.plot(steps, val_loss, label='Validation Loss', marker='o', color='orange')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(steps, accuracy, label='Validation Accuracy', marker='o', color='green')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy per Step')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WI_sMaVluQ9f",
    "outputId": "3774c067-8b02-45af-d622-4ff96283cec9"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and model from local path\n",
    "model_path = \"/content/bert_sentiment_model_verified\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Label mapping\n",
    "id2label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "# Define batch prediction function with full confidence scores\n",
    "def predict_multiple_sentiments_full_confidence(texts):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
    "            pred_id = np.argmax(probs)\n",
    "            result = {\n",
    "                \"text\": text,\n",
    "                \"predicted\": id2label[pred_id],\n",
    "                \"confidence_predicted\": round(probs[pred_id], 4),\n",
    "                \"conf_negative\": round(probs[0], 4),\n",
    "                \"conf_neutral\": round(probs[1], 4),\n",
    "                \"conf_positive\": round(probs[2], 4)\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "# Combined News Headlines\n",
    "texts = [\n",
    "\n",
    "    \"Israel accuses Hamas of trying to 'hide' the results of its airstrike which killed 'at least 90 people' while targeting October 7 mastermind Mohammed Deif - as the terrorist group insists its military chief is in good health\",\n",
    "    \"Global Economy Shows Signs of Recovery as Job Market Improves\",\n",
    "    \"Five Indian storytellers for captivating and entertaining storytelling\",\n",
    "    \"Promotion for infielder Brooks Baldwin among flurry of White Sox moves coming out of All-Star break\",\n",
    "    \"New Study Shows Exercise Boosts Mental Health and Well-Being\",\n",
    "\n",
    "    # New Positive\n",
    "    \"Tech Company Announces Record Profits in Q1\",\n",
    "    \"Scientists Discover New Renewable Energy Source\",\n",
    "    \"New Vaccine Shows 95% Efficacy in Trials\",\n",
    "\n",
    "    # New Neutral\n",
    "    \"City Council Schedules Meeting for Budget Discussion\",\n",
    "    \"Temperature Expected to Rise Slightly Over the Weekend\",\n",
    "    \"Annual Tech Conference Opens with Keynote Speech\",\n",
    "    \"Local Library Hosts Summer Reading Program\",\n",
    "\n",
    "    # New Negative\n",
    "    \"Earthquake in Turkey Leaves Hundreds Homeless\",\n",
    "\n",
    "    \"Oil Spill Threatens Marine Wildlife\",\n",
    "    \"Protesters Clash With Police Over New Law\",\n",
    "\n",
    "    # Tricky Positives (may be misclassified as Neutral)\n",
    "    \"Renewable energy jobs rise globally, says new report\",\n",
    "\n",
    "    # Tricky Neutrals (may be seen as Positive)\n",
    "    \"Scientists announce findings in solar panel efficiency study\",\n",
    "    \"NASA schedules new mission to study asteroid samples\",\n",
    "    \"University to host AI ethics conference next month\",\n",
    "\n",
    "    # Tricky Negatives (soft language, but serious events)\n",
    "    \"Inflation concerns persist despite economic recovery signs\",\n",
    "    \"Climate report warns of increasing natural disasters\",\n",
    "    \"War-torn region sees tentative ceasefire amid unrest\"\n",
    "]\n",
    "\n",
    "# Expected Sentiments\n",
    "expected_sentiments = [\n",
    "     \"Negative\", \"Positive\", \"Neutral\", \"Neutral\", \"Positive\",\n",
    "\n",
    "    # New Positive\n",
    "    \"Positive\", \"Positive\", \"Positive\",\n",
    "\n",
    "    # New Neutral\n",
    "    \"Neutral\", \"Neutral\", \"Neutral\", \"Neutral\",\n",
    "\n",
    "    # New Negative\n",
    "    \"Negative\", \"Negative\", \"Negative\",\n",
    "\n",
    "    # Tricky Positive\n",
    "     \"Positive\",\n",
    "\n",
    "    # Tricky Neutral\n",
    "    \"Neutral\", \"Neutral\", \"Neutral\",\n",
    "\n",
    "    # Tricky Negative\n",
    "    \"Negative\", \"Negative\", \"Negative\"\n",
    "]\n",
    "\n",
    "# Predict\n",
    "results = predict_multiple_sentiments_full_confidence(texts)\n",
    "\n",
    "# Format results\n",
    "df = pd.DataFrame(results)\n",
    "df[\"Expected\"] = expected_sentiments\n",
    "df = df[[\"text\", \"Expected\", \"predicted\", \"confidence_predicted\", \"conf_negative\", \"conf_neutral\", \"conf_positive\"]]\n",
    "df.columns = [\"Text (Shortened)\", \"Expected\", \"Predicted\", \"Conf (Pred)\", \"Conf (Neg)\", \"Conf (Neu)\", \"Conf (Pos)\"]\n",
    "df[\"Text (Shortened)\"] = df[\"Text (Shortened)\"].str.slice(0, 60) + \"...\"\n",
    "\n",
    "# Display in table format\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "2FjMNxcCy-OZ",
    "outputId": "44fc8cc9-b8b9-4114-e2d5-e0fdc117d58e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Load the dataset ---\n",
    "df = pd.read_csv(\"news_sentiment_corrected_verified_fully_balanced.csv\")\n",
    "\n",
    "# --- Clean and preprocess ---\n",
    "df = df.dropna(subset=[\"Description\", \"verified_sentiment\", \"Source\"])\n",
    "df[\"verified_sentiment\"] = df[\"verified_sentiment\"].str.lower()\n",
    "df[\"verified_sentiment\"] = df[\"verified_sentiment\"].map({\"negative\": 0, \"neutral\": 1, \"positive\": 2})\n",
    "\n",
    "# Optional: map back to label names for better plot readability\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "df[\"Sentiment Label\"] = df[\"verified_sentiment\"].map(label_map)\n",
    "\n",
    "# --- Group by Source and Sentiment ---\n",
    "sentiment_dist = df.groupby([\"Source\", \"Sentiment Label\"]).size().reset_index(name=\"Count\")\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=sentiment_dist, x=\"Source\", y=\"Count\", hue=\"Sentiment Label\")\n",
    "plt.title(\"Sentiment Distribution Across News Sources \", fontsize=16)\n",
    "plt.xlabel(\"News Source\", fontsize=12)\n",
    "plt.ylabel(\"Article Count\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend(title=\"Sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rF0hDUElRCGQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
